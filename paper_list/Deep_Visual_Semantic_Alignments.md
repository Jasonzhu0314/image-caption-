# Deep Visual-Semantic Alignments for Generating Image Descriptions
## 1. 主要思想
利用图片-文本数据集通过对待文本作为弱标签，词的临近的分割等于一些特定成分，但是我们不知道文本在图片中的定位。我们的方法是推断这些对齐，并使用去训练一个文本的生成模型，
（1）我们设计一个深度神经网络去推断句子分割和图片区域。我们的模型通过多模型嵌入空间和一个结构化目标将两个形体连接起来。我们在图文检索实验上评估这种方法的有效性，并达到了STOA

生成文本与以前论文的区别：
以前的一部分方法生成image caption都是基于固定的模板，固定的模板都是那种基于图片的内容或者生成语法，但是这种方法限制了输出的多样性。

## 2. 我们的模型
我们介绍一个模型，对齐文本片段到视觉区域，并通过多模型嵌入描述他们，在训练的时候将对齐的文本用于第二阶段的训练，
描述图片（representing image）：
使用RCNN，使用预训练过后的CNN模型，将一张图片转化为4096维的向量。
描述句子（representing sentences）：
这点类似于机器翻译，直接翻译的话相当于没有考虑语言中句子的顺序，和词内容信息，为了解决这个问题，我们提出了双向RNN，每个词被定义为字典中的one-hot向量，加入已经训练好的300维的word-embedding，在训练的时候被固定，BRNN的输出St是词的位置和它周围的内容的一个函数，技术上说，每一个St代表了一句话中的整个句子，但是我们实验发现，这个词表示St和word在位置II的视觉概念对齐。
对齐目标：




