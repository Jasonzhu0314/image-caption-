# Explain Images with Multimodal Recurrent Neural Network
疑问：为什么用一层代表呢
log-Bilinear已经被引用了很多次
文中提到的以前的语言模型是指，采用一种语法的特定规则，他们将句子分为一些部分，这个部分对应图片中的一些目标和属性，这种形式生成的句子语法上是正确的。

## 1. 解决的内容：
它直接对基于图片和一句话以前的文字信息生成下一个文字的条件概率分布进行了建模，在更新权重的时候，梯度会反传给三个子网络，相当于将文字信息，wordembedding和图像信息融入到一个空间中，这样的话会使图片的信息和文字的信息在这个空间中有相近的表达。

在RNN中输入部分代替原来的拼接隐藏层和文字信息，改为隐藏层和文字信息的和


## 1. 简介
提取句子和句子和图片的特征，将他们映射到相同过的语义编码空间。mRNN可同样的应用于图片生成文字，而且也可以用于图片和句子的检索。
mRNN由三个部分组成，语言模型部分，图片部分和多模型部分

### 1.1 缺点
只能查询到以前出现在数据集中的图片的句子表示，因为在预测的时候，所使用的字典都是数据集中的单词组成的，所以说只能生产字典中的字，

## 2. 主要结构
与其他论文的不同点：

（1）加入了两层的词嵌入，输入一开始为one-hot编码

（2）在预测每个词的时候，通常的结构是将图像的部分加在RNN的输入中，但是这个只是将结构加在最后的预测部分，与词嵌入的部分，RNN的输出和图像feature map

（3）不使用预训练的word embedding模型

（4）计算loss使用常见的条件概率交叉熵，计算一句话中词的交叉熵损失，在预测时候前后分别加上START和END

（5）论文中在训练的时候固定了图片的特征提取层（AlexNet），但是说在后面的任务中会加入微调CNN网络



